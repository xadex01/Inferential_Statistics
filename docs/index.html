<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Inferential Statistics</title>
  <meta name="description" content="Inferential Statistics" />
  <meta name="generator" content="bookdown 0.38 and GitBook 2.6.7" />

  <meta property="og:title" content="Inferential Statistics" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Inferential Statistics" />
  
  
  

<meta name="author" content="AbdulHafiz Abba" />


<meta name="date" content="2024-03-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="neyman-pearson-lemma.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inferential Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#definitions"><i class="fa fa-check"></i><b>1.1.1</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>1.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="neyman-pearson-lemma.html"><a href="neyman-pearson-lemma.html"><i class="fa fa-check"></i><b>2</b> Neyman-Pearson Lemma</a>
<ul>
<li class="chapter" data-level="2.1" data-path="neyman-pearson-lemma.html"><a href="neyman-pearson-lemma.html#the-neyman-pearson-lemma"><i class="fa fa-check"></i><b>2.1</b> The Neyman Pearson Lemma</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-caucus-race-and-a-long-tale.html"><a href="a-caucus-race-and-a-long-tale.html"><i class="fa fa-check"></i><b>3</b> A caucus-race and a long tale</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Inferential Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Inferential Statistics</h1>
<p class="author"><em>AbdulHafiz Abba</em></p>
<p class="date"><em>2024-03-12</em></p>
</div>
<div id="point-estimation" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Point Estimation<a href="index.html#point-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="overview" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Overview<a href="index.html#overview" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we have an unknown population parameter, such as a population mean <span class="math inline">\(\mu\)</span> or a population proportion <span class="math inline">\(p\)</span>, which we’d like to estimate. For example, suppose we are interested in estimating:</p>
<ul>
<li><span class="math inline">\(p\)</span>: the (unknown) proportion of American college students, 18-24, who have a smartphone</li>
<li><span class="math inline">\(\mu\)</span>: the (unknown) mean number of days it takes Alzheimer’s patients to achieve certain milestones</li>
</ul>
<p>In either case, we can’t possibly survey the entire population. That is, we can’t survey all American college students between the ages of 18 and 24. Nor can we survey all patients with Alzheimer’s disease. So, of course, we take a random sample from the population and use the resulting data to estimate the value of the population parameter. However, we want the estimate to be “good” in some way.</p>
<p>In this lesson, we’ll learn two methods, namely the method of maximum likelihood and the method of moments, for deriving formulas for “good” point estimates for population parameters. We’ll also learn one way of assessing whether a point estimate is “good.” We’ll do that by defining what it means for an estimate to be unbiased.</p>
<div id="definitions" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Definitions<a href="index.html#definitions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ll start the lesson with some formal definitions. In doing so, recall that we denote the <span class="math inline">\(n\)</span> random variables arising from a random sample as subscripted uppercase letters: <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>. The corresponding observed values of a specific random sample are then denoted as subscripted lowercase letters: <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>.</p>
<p><strong>Parameter Space</strong></p>
<p>The range of possible values of the parameter <span class="math inline">\(\theta\)</span> is called the parameter space <span class="math inline">\(\Omega\)</span> (the Greek letter “omega”). For example, if <span class="math inline">\(\mu\)</span> denotes the mean grade point average of all college students, then the parameter space (assuming a 4-point grading scale) is:
<span class="math display">\[ \Omega = \{ \mu : 0 \leq \mu \leq 4 \} \]</span>
And, if <span class="math inline">\(p\)</span> denotes the proportion of students who smoke cigarettes, then the parameter space is:
<span class="math display">\[ \Omega = \{ p : 0 \leq p \leq 1 \} \]</span></p>
<p><strong>Point Estimator</strong></p>
<p>The function of <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>, that is, the statistic <span class="math inline">\(u = (X_1, X_2, \ldots, X_n)\)</span> used to estimate <span class="math inline">\(\theta\)</span> is called a point estimator of <span class="math inline">\(\theta\)</span>. For example, the function:
<span class="math display">\[ \bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i \]</span>
is a point estimator of the population mean <span class="math inline">\(\mu\)</span>. The function:
<span class="math display">\[ \hat{p} = \frac{1}{n} \sum_{i=1}^{n} X_i \]</span>
(where <span class="math inline">\(X_i = 0\)</span> or 1) is a point estimator of the population proportion <span class="math inline">\(p\)</span>. And, the function:
<span class="math display">\[ S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2 \]</span>
is a point estimator of the population variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p><strong>Point Estimate</strong></p>
<p>The function <span class="math inline">\(u(x_1, x_2, \ldots, x_n)\)</span> computed from a set of data is an observed point estimate of <span class="math inline">\(\theta\)</span>. For example, if <span class="math inline">\(x_i\)</span> are the observed grade point averages of a sample of 88 students, then:
<span class="math display">\[ \bar{x} = \frac{1}{88} \sum_{i=1}^{88} x_i = 3.12 \]</span>
is a point estimate of <span class="math inline">\(\mu\)</span>, the mean grade point average of all the students in the population. And, if <span class="math inline">\(x_i = 0\)</span> if a student has no tattoo, and <span class="math inline">\(x_i = 1\)</span> if a student has a tattoo, then:
<span class="math display">\[ \hat{p} = 0.11 \]</span>
is a point estimate of <span class="math inline">\(p\)</span>, the proportion of all students in the population who have a tattoo.</p>
<p>Now, with the above definitions aside, let’s go learn about the method of maximum likelihood.</p>
</div>
</div>
<div id="maximum-likelihood-estimation" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Maximum Likelihood Estimation<a href="index.html#maximum-likelihood-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Statement of the Problem</strong></p>
<p>Suppose we have a random sample <span class="math inline">\(X_1, X_2, ..., X_n\)</span> whose assumed probability distribution depends on some unknown parameter <span class="math inline">\(\theta\)</span>. Our primary goal here will be to find a point estimator <span class="math inline">\(u(X_1, X_2, ..., X_n)\)</span>, such that <span class="math inline">\(u(x_1, x_2, ..., x_n)\)</span> is a “good” point estimate of <span class="math inline">\(\theta\)</span>, where <span class="math inline">\(x_1, x_2, ..., x_n\)</span> are the observed values of the random sample. For example, if we plan to take a random sample <span class="math inline">\(X_1, X_2, ..., X_n\)</span> for which the <span class="math inline">\(X_i\)</span> are assumed to be normally distributed with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then our goal will be to find a good estimate of <span class="math inline">\(\mu\)</span>, say, using the data <span class="math inline">\(x_1, x_2, ..., x_n\)</span> that we obtained from our specific random sample.</p>
<p><strong>The Basic Idea</strong></p>
<p>It seems reasonable that a good estimate of the unknown parameter <span class="math inline">\(\theta\)</span> would be the value of <span class="math inline">\(\theta\)</span> that maximizes the probability, or rather, the likelihood, of getting the data we observed. So, that is, in a nutshell, the idea behind the method of maximum likelihood estimation. But how would we implement the method in practice? Well, suppose we have a random sample <span class="math inline">\(X_1, X_2, ..., X_n\)</span> for which the probability density (or mass) function of each <span class="math inline">\(X_i\)</span> is <span class="math inline">\(f(x_i;\theta)\)</span>. Then, the joint probability mass (or density) function of <span class="math inline">\(X_1, X_2, ..., X_n\)</span>, which we’ll (not so arbitrarily) call <span class="math inline">\(L(\theta)\)</span> is:</p>
<p><span class="math display">\[L(\theta) = P(X_1=x_1, X_2=x_2, ..., X_n=x_n) = f(x_1;\theta) \cdot f(x_2;\theta) \cdot ... \cdot f(x_n;\theta) = \prod_{i=1}^n f(x_i;\theta)\]</span></p>
<p>In light of the basic idea of maximum likelihood estimation, one reasonable way to proceed is to treat the “likelihood function” <span class="math inline">\(L(\theta)\)</span> as a function of <span class="math inline">\(\theta\)</span>, and find the value of <span class="math inline">\(\theta\)</span> that maximizes it.</p>
<p><strong>Example 1-1</strong></p>
<p>Suppose we have a random sample <span class="math inline">\(X_1, X_2, ..., X_n\)</span> where:</p>
<ul>
<li><span class="math inline">\(X_i = 0\)</span> if a randomly selected student does not own a sports car, and</li>
<li><span class="math inline">\(X_i = 1\)</span> if a randomly selected student does own a sports car.</li>
</ul>
<p>Assuming that the <span class="math inline">\(X_i\)</span> are independent Bernoulli random variables with unknown parameter <span class="math inline">\(p\)</span>, find the maximum likelihood estimator of <span class="math inline">\(p\)</span>, the proportion of students who own a sports car.</p>
<p><strong>Answer</strong></p>
<p>If the <span class="math inline">\(X_i\)</span> are independent Bernoulli random variables with unknown parameter <span class="math inline">\(p\)</span>, then the probability mass function of each <span class="math inline">\(X_i\)</span> is:</p>
<p><span class="math display">\[f(x_i;\theta) = p^{x_i} \cdot (1-p)^{1-x_i}\]</span></p>
<p>for <span class="math inline">\(x_i = 0\)</span> or 1 and <span class="math inline">\(0 &lt; p &lt; 1\)</span>. Therefore, the likelihood function <span class="math inline">\(L(p)\)</span> is, by definition:</p>
<p><span class="math display">\[L(p) = \prod_{i=1}^n f(x_i;\theta) = p^{x_1} \cdot (1-p)^{1-x_1} \cdot p^{x_2} \cdot (1-p)^{1-x_2} \cdot ... \cdot p^{x_n} \cdot (1-p)^{1-x_n}\]</span></p>
<p>for <span class="math inline">\(0 &lt; p &lt; 1\)</span>. Simplifying, by summing up the exponents, we get:</p>
<p><span class="math display">\[L(p) = p^{\sum x_i} \cdot (1-p)^{n - \sum x_i}\]</span></p>
<p>Now, in order to implement the method of maximum likelihood, we need to find the <span class="math inline">\(p\)</span> that maximizes the likelihood <span class="math inline">\(L(p)\)</span>. We need to put on our calculus hats now since, in order to maximize the function, we are going to need to differentiate the likelihood function with respect to <span class="math inline">\(p\)</span>. In doing so, we’ll use a “trick” that often makes the differentiation a bit easier. Note that the natural logarithm is an increasing function of <span class="math inline">\(x\)</span>:</p>
<div class="figure">
<img src="path/to/image.jpg" alt="" />
<p class="caption">Alt Text</p>
</div>
<p>That is, if <span class="math inline">\(x_1 &lt; x_2\)</span>, then <span class="math inline">\(f(x_1) &lt; f(x_2)\)</span>. That means that the value of <span class="math inline">\(p\)</span> that maximizes the natural logarithm of the likelihood function <span class="math inline">\(\ln L(p)\)</span> is also the value of <span class="math inline">\(p\)</span> that maximizes the likelihood function <span class="math inline">\(L(p)\)</span>. So, the “trick” is to take the derivative of <span class="math inline">\(\ln L(p)\)</span> (with respect to <span class="math inline">\(p\)</span>) rather than taking the derivative of <span class="math inline">\(L(p)\)</span>. Again, doing so often makes the differentiation much easier. (By the way, throughout the remainder of this course, I will use either <span class="math inline">\(\ln L(p)\)</span> or <span class="math inline">\(\log L(p)\)</span> to denote the natural logarithm of the likelihood function.)</p>
<p>In this case, the natural logarithm of the likelihood function is:</p>
<p><span class="math display">\[ \log L(p) = \left( \sum_{i=1}^{n} x_i \right) \log(p) + \left( n - \sum_{i=1}^{n} x_i \right) \log(1-p) \]</span></p>
<p>Now, taking the derivative of the log-likelihood, and setting it to <span class="math inline">\(0\)</span>, we get:</p>
<p><span class="math display">\[ \frac{\partial \log L(p)}{\partial p} = \frac{\sum_{i=1}^{n} x_i}{p} - \frac{n - \sum_{i=1}^{n} x_i}{1-p} \]</span></p>
<p>Now, multiplying through by <span class="math inline">\(p(1-p)\)</span>, we get:</p>
<p><span class="math display">\[ \left( \sum_{i=1}^{n} x_i \right) (1-p) - (n - \sum_{i=1}^{n} x_i) p = 0 \]</span></p>
<p>Upon distribution, we see that two of the resulting terms cancel each other out:</p>
<p><span class="math display">\[ \sum_{i=1}^{n} x_i - p \sum_{i=1}^{n} x_i - np + p \sum_{i=1}^{n} x_i = 0 \]</span></p>
<p>leaving us with:</p>
<p><span class="math display">\[ \sum_{i=1}^{n} x_i - np = 0 \]</span></p>
<p>Now, all we have to do is solve for <span class="math inline">\(p\)</span>. In doing so, you’ll want to make sure that you always put a hat (“^”) on the parameter, in this case, <span class="math inline">\(p\)</span>, to indicate it is an estimate:</p>
<p><span class="math display">\[ \hat{p} = \frac{\sum_{i=1}^{n} x_i}{n} \]</span></p>
<p>or, alternatively, an estimator:</p>
<p><span class="math display">\[ \hat{p} = \frac{\sum_{i=1}^{n} X_i}{n} \]</span></p>
<p>Oh, and we should technically verify that we indeed did obtain a maximum. We can do that by verifying that the second derivative of the log-likelihood with respect to <span class="math inline">\(p\)</span> is negative. It is, but you might want to do the work to convince yourself!</p>
<p>Now, with that example behind us, let us take a look at formal definitions of the terms:</p>
<ul>
<li>Likelihood Function</li>
<li>Maximum Likelihood Estimators</li>
<li>Maximum Likelihood Estimates</li>
</ul>
<p><strong>Definition:</strong> Let <span class="math inline">\(X_1, X_2, ..., X_n\)</span> be a random sample from a distribution that depends on one or more unknown parameters <span class="math inline">\(\theta_1, \theta_2, ..., \theta_m\)</span> with probability density (or mass) function <span class="math inline">\(f(x_i; \theta_1, \theta_2, ..., \theta_m)\)</span>. Suppose that <span class="math inline">\(\theta_1, \theta_2, ..., \theta_m\)</span> is restricted to a given parameter space <span class="math inline">\(\Omega\)</span>. Then:</p>
<ul>
<li>When regarded as a function of <span class="math inline">\(\theta_1, \theta_2, ..., \theta_m\)</span>, the joint probability density (or mass) function of <span class="math inline">\(X_1, X_2, ..., X_n\)</span>:</li>
</ul>
<p><span class="math display">\[ L(\theta_1, \theta_2, ..., \theta_m) = \prod_{i=1}^{n} f(x_i; \theta_1, \theta_2, ..., \theta_m) \]</span></p>
<p><span class="math inline">\((\theta_1, \theta_2, ..., \theta_m)\)</span> in <span class="math inline">\(\Omega\)</span> is called the likelihood function.</p>
<ul>
<li>If <span class="math inline">\([u_1(x_1, x_2, ..., x_n), u_2(x_1, x_2, ..., x_n), ..., u_m(x_1, x_2, ..., x_n)]\)</span> is the m-tuple that maximizes the likelihood function, then:</li>
</ul>
<p><span class="math display">\[ \hat{\theta}_i = u_i(X_1, X_2, ..., X_n) \]</span></p>
<p>is the maximum likelihood estimator of <span class="math inline">\(\theta_i\)</span>, for <span class="math inline">\(i = 1,2,...,m\)</span>.</p>
<ul>
<li>The corresponding observed values of the statistics in (2), namely:</li>
</ul>
<p><span class="math display">\[ [u_1(x_1, x_2, ..., x_n), u_2(x_1, x_2, ..., x_n), ..., u_m(x_1, x_2, ..., x_n)] \]</span></p>
<p>are called the maximum likelihood estimates of <span class="math inline">\(\theta_i\)</span>, for <span class="math inline">\(i = 1,2,...,m\)</span>.</p>
<p><strong>Example 1-2:</strong>
Suppose the weights of randomly selected American female college students are normally distributed with unknown mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. A random sample of 10 American female college students yielded the following weights (in pounds): 115, 122, 130, 127, 149, 160, 152, 138, 149, 180.</p>
<p>Based on the definitions given above, identify the likelihood function and the maximum likelihood estimator of <span class="math inline">\(\mu\)</span>, the mean weight of all American female college students. Using the given sample, find a maximum likelihood estimate of <span class="math inline">\(\mu\)</span> as well.</p>
<p><strong>Answer:</strong>
The probability density function of <span class="math inline">\(X_i\)</span> is:</p>
<p><span class="math display">\[ f(x_i; \mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{(x_i - \mu)^2}{2\sigma^2}\right] \]</span></p>
<p>for <span class="math inline">\(-\infty &lt; x &lt; \infty\)</span>. The parameter space is <span class="math inline">\(\Omega = \{ (\mu, \sigma) : -\infty &lt; \mu &lt; \infty \text{ and } 0 &lt; \sigma &lt; \infty \}\)</span>. Therefore, the likelihood function is:</p>
<p><span class="math display">\[ L(\mu, \sigma) = \sigma^{-n} (2\pi)^{-\frac{n}{2}} \exp\left[-\frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2\right] \]</span></p>
<p>for <span class="math inline">\(-\infty &lt; \mu &lt; \infty\)</span> and <span class="math inline">\(0 &lt; \sigma &lt; \infty\)</span>. It can be shown, upon maximizing the likelihood function with respect to <span class="math inline">\(\mu\)</span>, that the maximum likelihood estimator of <span class="math inline">\(\mu\)</span> is:</p>
<p><span class="math display">\[ \hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} X_i = \bar{X} \]</span></p>
<p>Based on the given sample, a maximum likelihood estimate of <span class="math inline">\(\mu\)</span> is:</p>
<p><span class="math display">\[ \hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} x_i = \frac{1}{10} (115 + \ldots + 180) = 142.2 \]</span></p>
<p>pounds. Note that the only difference between the formulas for the maximum likelihood estimator and the maximum likelihood estimate is that:</p>
<ul>
<li>the estimator is defined using capital letters (to denote that its value is random), and</li>
<li>the estimate is defined using lowercase letters (to denote that its value is fixed and based on an obtained sample).</li>
</ul>
<p>Okay, so now we have the formal definitions out of the way. The first example on this page involved a joint probability mass function that depends on only one parameter, namely P, the proportion of successes. Now, let’s take a look at an example that involves a joint probability density function that depends on two parameters.</p>
<p><strong>Example 1-3:</strong>
Let <span class="math inline">\(X_1, X_2, ..., X_n\)</span> be a random sample from a normal distribution with unknown mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Find maximum likelihood estimators of mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p><strong>Answer:</strong>
In finding the estimators, the first thing we’ll do is write the probability density function as a function of <span class="math inline">\(\theta_1 = \mu\)</span> and <span class="math inline">\(\theta_2 = \sigma^2\)</span>:</p>
<p><span class="math display">\[ f(x_i; \theta_1, \theta_2) = \frac{1}{\sqrt{\theta_2} \sqrt{2\pi}} \exp\left[-\frac{(x_i - \theta_1)^2}{2\theta_2}\right] \]</span></p>
<p>for <span class="math inline">\(-\infty &lt; \theta_1 &lt; \infty\)</span> and <span class="math inline">\(0 &lt; \theta_2 &lt; \infty\)</span>. We do this so as not to cause confusion when taking the derivative of the likelihood with respect to <span class="math inline">\(\sigma^2\)</span>. Now, that makes the likelihood function:</p>
<p><span class="math display">\[ L(\theta_1, \theta_2) = \prod_{i=1}^{n} f(x_i; \theta_1, \theta_2) = \theta_2^{-\frac{n}{2}} (2\pi)^{-\frac{n}{2}} \exp\left[-\frac{1}{2\theta_2} \sum_{i=1}^{n} (x_i - \theta_1)^2 \right] \]</span></p>
<p>and therefore the log of the likelihood function:</p>
<p><span class="math display">\[ \log L(\theta_1, \theta_2) = -\frac{n}{2} \log \theta_2 - \frac{n}{2} \log(2\pi) - \frac{\sum{(x_i - \theta_1)^2}}{2\theta_2} \]</span></p>
<p>Now, upon taking the partial derivative of the log likelihood with respect to <span class="math inline">\(\theta_1\)</span>, and setting to 0, we see that a few things cancel each other out, leaving us with:</p>
<p><span class="math display">\[ \frac{\partial \log L(\theta_1, \theta_2)}{\partial \theta_1} = \frac{-2\sum{(x_i - \theta_1)}(-1)}{2\theta_2} \stackrel{\text{set}}{=} 0 \]</span></p>
<p>Now, multiplying through by <span class="math inline">\(\theta_2\)</span>, and distributing the summation, we get:</p>
<p><span class="math display">\[ \sum{x_i - n\theta_1} = 0 \]</span></p>
<p>Now, solving for <span class="math inline">\(\theta_1\)</span>, and putting on its hat, we have shown that the maximum likelihood estimate of <span class="math inline">\(\theta_1\)</span> is:</p>
<p><span class="math display">\[ \hat{\theta}_1 = \hat{\mu} = \frac{\sum{x_i}}{n} = \bar{x} \]</span></p>
<p>Now for <span class="math inline">\(\theta_2\)</span>. Taking the partial derivative of the log likelihood with respect to <span class="math inline">\(\theta_2\)</span>, and setting to 0, we get:</p>
<p><span class="math display">\[ \frac{\partial \log L(\theta_1, \theta_2)}{\partial \theta_2} = \frac{-n}{2\theta_2} + \frac{\sum{(x_i - \theta_1)^2}}{2\theta_2^2} \stackrel{\text{set}}{=} 0 \]</span></p>
<p>Multiplying through by <span class="math inline">\(2\theta_2^2\)</span>, we get:</p>
<p><span class="math display">\[ -n\theta_2 + \sum{(x_i - \theta_1)^2} = 0 \]</span></p>
<p>And, solving for <span class="math inline">\(\theta_2\)</span>, and putting on its hat, we have shown that the maximum likelihood estimate of <span class="math inline">\(\theta_2\)</span> is:</p>
<p><span class="math display">\[ \hat{\theta}_2 = \hat{\sigma}^2 = \frac{\sum{(x_i - \theta_1)^2}}{n} \]</span></p>
<p>(I’ll again leave it to you to verify, in each case, that the second partial derivative of the log likelihood is negative, and therefore that we did indeed find maxima.)</p>
<p>In summary, we have shown that the maximum likelihood estimators of <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> for the normal model are:</p>
<p><span class="math display">\[ \hat{\mu} = \left(\sum{X_i}\right)/n = \bar{X} \]</span></p>
<p>and</p>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{\sum{(X_i - \bar{X})^2}}{n} \]</span></p>
<p>respectively.</p>
<p>Note that the maximum likelihood estimator of <span class="math inline">\(\sigma^2\)</span> for the normal model is not the sample variance <span class="math inline">\(S^2\)</span>. They are, in fact, competing estimators. So how do we know which estimator we should use for <span class="math inline">\(\sigma^2\)</span>? Well, one way is to choose the estimator that is “unbiased.” Let’s go learn about unbiased estimators.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="neyman-pearson-lemma.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/xadex01/Inferential_Statistics/edit/main/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/xadex01/Inferential_Statistics/blob/main/index.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

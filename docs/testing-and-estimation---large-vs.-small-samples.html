<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Testing and Estimation - Large vs. Small Samples | Inferential Statistics</title>
  <meta name="description" content="Chapter 3 Testing and Estimation - Large vs. Small Samples | Inferential Statistics" />
  <meta name="generator" content="bookdown 0.38 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Testing and Estimation - Large vs. Small Samples | Inferential Statistics" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Testing and Estimation - Large vs. Small Samples | Inferential Statistics" />
  
  
  

<meta name="author" content="AbdulHafiz Abba" />


<meta name="date" content="2024-03-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="neyman-pearson-lemma.html"/>
<link rel="next" href="understanding-the-poisson-distribution.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inferential Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#definitions"><i class="fa fa-check"></i><b>1.1.1</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>1.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="neyman-pearson-lemma.html"><a href="neyman-pearson-lemma.html"><i class="fa fa-check"></i><b>2</b> Neyman-Pearson Lemma</a>
<ul>
<li class="chapter" data-level="2.1" data-path="neyman-pearson-lemma.html"><a href="neyman-pearson-lemma.html#the-neyman-pearson-lemma"><i class="fa fa-check"></i><b>2.1</b> The Neyman Pearson Lemma</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html"><i class="fa fa-check"></i><b>3</b> Testing and Estimation - Large vs. Small Samples</a>
<ul>
<li class="chapter" data-level="3.1" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html#central-limit-theorem-clt"><i class="fa fa-check"></i><b>3.1</b> Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="3.2" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html#conceptual-explanation"><i class="fa fa-check"></i><b>3.2</b> Conceptual Explanation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html#understanding-the-binomial-distribution"><i class="fa fa-check"></i><b>3.2.1</b> Understanding the Binomial Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html#testing-large-samples"><i class="fa fa-check"></i><b>3.3</b> Testing Large Samples</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html#z-test-for-proportions"><i class="fa fa-check"></i><b>3.3.1</b> Z-Test for Proportions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html#estimation-of-parameters"><i class="fa fa-check"></i><b>3.4</b> Estimation of Parameters</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html#point-estimation-1"><i class="fa fa-check"></i><b>3.4.1</b> Point Estimation</a></li>
<li class="chapter" data-level="3.4.2" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html#interval-estimation"><i class="fa fa-check"></i><b>3.4.2</b> Interval Estimation</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html#small-sample-situations"><i class="fa fa-check"></i><b>3.5</b> Small Sample Situations</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html#binomial-test"><i class="fa fa-check"></i><b>3.5.1</b> Binomial Test</a></li>
<li class="chapter" data-level="3.5.2" data-path="testing-and-estimation---large-vs.-small-samples.html"><a href="testing-and-estimation---large-vs.-small-samples.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.5.2</b> Chi-Square Goodness-of-Fit Test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="understanding-the-poisson-distribution.html"><a href="understanding-the-poisson-distribution.html"><i class="fa fa-check"></i><b>4</b> Understanding the Poisson Distribution</a>
<ul>
<li class="chapter" data-level="4.1" data-path="understanding-the-poisson-distribution.html"><a href="understanding-the-poisson-distribution.html#testing-large-samples-for-poisson-distribution"><i class="fa fa-check"></i><b>4.1</b> Testing Large Samples for Poisson Distribution</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="understanding-the-poisson-distribution.html"><a href="understanding-the-poisson-distribution.html#testing-large-samples-1"><i class="fa fa-check"></i><b>4.1.1</b> Testing Large Samples</a></li>
<li class="chapter" data-level="4.1.2" data-path="understanding-the-poisson-distribution.html"><a href="understanding-the-poisson-distribution.html#test-statistic"><i class="fa fa-check"></i><b>4.1.2</b> Test Statistic</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Inferential Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="testing-and-estimation---large-vs.-small-samples" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Testing and Estimation - Large vs. Small Samples<a href="testing-and-estimation---large-vs.-small-samples.html#testing-and-estimation---large-vs.-small-samples" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This lecture note explores the concepts of testing and estimation in the context of large and small samples.</p>
<div id="central-limit-theorem-clt" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Central Limit Theorem (CLT)<a href="testing-and-estimation---large-vs.-small-samples.html#central-limit-theorem-clt" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Central Limit Theorem (CLT) is a fundamental concept in statistics that describes the behavior of sample means when drawing repeated samples from a population. It states that, regardless of the shape of the population distribution, the distribution of sample means will tend to be approximately normal as the sample size increases.</p>
</div>
<div id="conceptual-explanation" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Conceptual Explanation<a href="testing-and-estimation---large-vs.-small-samples.html#conceptual-explanation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The CLT is based on three key principles:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Sampling Distribution</strong>: When we draw multiple samples from a population and calculate the mean of each sample, we create a sampling distribution of the sample means.</p></li>
<li><p><strong>Approximate Normality</strong>: The distribution of sample means will be approximately normal if the sample size is sufficiently large, regardless of the shape of the population distribution. This is particularly true for sample sizes greater than 30.</p></li>
<li><p><strong>Mean and Standard Deviation</strong>: The mean of the sample means will be approximately equal to the population mean, and the standard deviation of the sample means (standard error) will decrease as the sample size increases.</p></li>
</ol>
<p><strong>Example 1</strong>
Suppose we want to understand the distribution of the mean height of students in the campus of Gadau. We’ll collect the heights of 10000 students as our population data.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="testing-and-estimation---large-vs.-small-samples.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate student heights in Gadau campus</span></span>
<span id="cb1-2"><a href="testing-and-estimation---large-vs.-small-samples.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-3"><a href="testing-and-estimation---large-vs.-small-samples.html#cb1-3" aria-hidden="true" tabindex="-1"></a>student_heights <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="at">mean =</span> <span class="dv">170</span>, <span class="at">sd =</span> <span class="dv">10</span>)</span>
<span id="cb1-4"><a href="testing-and-estimation---large-vs.-small-samples.html#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="testing-and-estimation---large-vs.-small-samples.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Display summary statistics of student heights</span></span>
<span id="cb1-6"><a href="testing-and-estimation---large-vs.-small-samples.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(student_heights)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   141.9   163.7   170.1   170.2   176.6   202.4</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of samples</span></span>
<span id="cb3-2"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-2" aria-hidden="true" tabindex="-1"></a>num_samples <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb3-3"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample size</span></span>
<span id="cb3-4"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-4" aria-hidden="true" tabindex="-1"></a>sample_size <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb3-5"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize a vector to store sample means</span></span>
<span id="cb3-7"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-7" aria-hidden="true" tabindex="-1"></a>sample_means <span class="ot">&lt;-</span> <span class="fu">numeric</span>(num_samples)</span>
<span id="cb3-8"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Take random samples and calculate means</span></span>
<span id="cb3-10"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_samples) {</span>
<span id="cb3-11"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-11" aria-hidden="true" tabindex="-1"></a>  sample <span class="ot">&lt;-</span> <span class="fu">sample</span>(student_heights, sample_size)</span>
<span id="cb3-12"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-12" aria-hidden="true" tabindex="-1"></a>  sample_means[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample)</span>
<span id="cb3-13"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-14"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distribution of sample means</span></span>
<span id="cb3-16"><a href="testing-and-estimation---large-vs.-small-samples.html#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(sample_means, <span class="at">breaks =</span> <span class="dv">30</span>, <span class="at">main =</span> <span class="st">&quot;Distribution of Sample Means&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Sample Mean Height (cm)&quot;</span>, <span class="at">col =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">border =</span> <span class="st">&quot;white&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/gather_data-1.png" width="672" />
In the histogram above, we can observe that as the number of samples increases, the distribution of sample means becomes more symmetric and bell-shaped, resembling a normal distribution. This is consistent with the Central Limit Theorem.</p>
<p><strong>Example 2: Rolling a Fair Die</strong></p>
<p>Suppose we want to understand the distribution of the mean of rolling a fair six-sided die. We will simulate rolling the die 1000 times and calculate the mean for each set of rolls.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb4-2"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb4-3"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate rolling a fair six-sided die 1000 times</span></span>
<span id="cb4-5"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-5" aria-hidden="true" tabindex="-1"></a>num_rolls <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb4-6"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-6" aria-hidden="true" tabindex="-1"></a>die_rolls <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, num_rolls, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-7"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean of each set of rolls</span></span>
<span id="cb4-9"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-9" aria-hidden="true" tabindex="-1"></a>sample_means <span class="ot">&lt;-</span> <span class="fu">numeric</span>(num_rolls)</span>
<span id="cb4-10"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_rolls) {</span>
<span id="cb4-11"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-11" aria-hidden="true" tabindex="-1"></a>  sample_means[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">sample</span>(die_rolls, i))</span>
<span id="cb4-12"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-13"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distribution of sample means</span></span>
<span id="cb4-15"><a href="testing-and-estimation---large-vs.-small-samples.html#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(sample_means, <span class="at">breaks =</span> <span class="dv">30</span>, <span class="at">main =</span> <span class="st">&quot;Distribution of Sample Means&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Sample Mean&quot;</span>, <span class="at">col =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">border =</span> <span class="st">&quot;white&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/example_code-1.png" width="672" />
## Testing and Estimation of Binomial Distribution</p>
<div id="understanding-the-binomial-distribution" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Understanding the Binomial Distribution<a href="testing-and-estimation---large-vs.-small-samples.html#understanding-the-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before we proceed, let’s quickly recap what the binomial distribution entails. The binomial distribution describes the probability of a certain number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. It is characterized by two parameters: the number of trials, denoted as <span class="math inline">\(n\)</span>, and the probability of success in each trial, denoted as <span class="math inline">\(p\)</span>.</p>
<p><strong>Example Scenario</strong></p>
<ul>
<li><p><strong>Dice Rolls:</strong> Rolling a regular six-sided die multiple times. The binomial distribution helps us predict how many times we might roll a specific number, like getting exactly three 6s out of ten rolls.</p></li>
<li><p><strong>Light Switch:</strong> Turning a light switch on and off. Each time you flip the switch, it’s like a trial where you can either get light (success) or darkness (failure). The binomial distribution helps us estimate the probability of getting a certain number of successes (light) in a series of trials.</p></li>
<li><p><strong>Bowling Pins:</strong> Knocking down bowling pins with a ball. Each time you roll the ball, you either knock down some pins or miss completely. The binomial distribution helps us calculate the likelihood of knocking down a certain number of pins in a given number of attempts.</p></li>
<li><p><strong>Basketball Shots:</strong> Shooting basketballs into a hoop. Each shot can either go in (success) or miss (failure). The binomial distribution can help us figure out the chances of making a certain number of shots out of several attempts.</p></li>
</ul>
<p><strong>Example of a Binomial Distribution</strong></p>
<p>Suppose in a university, 70% of students pass a certain course on their first attempt. If we randomly select 20 students from the university, what is the probability that exactly 15 of them will pass the course on their first attempt?</p>
<p>Using the binomial distribution, the probability of exactly 15 students passing the course is given by:</p>
<p><span class="math display">\[
P(X = 15) = \binom{20}{15} (0.70)^{15} (1 - 0.70)^{20 - 15}
\]</span></p>
Where:
<p>Plugging in the values:</p>
<p><span class="math display">\[
P(X = 15) = \binom{20}{15} (0.70)^{15} (0.30)^{5}
\]</span></p>
<p>Calculating:</p>
<p><span class="math display">\[
P(X = 15) = \frac{20!}{15! \times (20 - 15)!} (0.70)^{15} (0.30)^{5}
\]</span></p>
<p><span class="math display">\[
P(X = 15) = \frac{20!}{15! \times 5!} (0.70)^{15} (0.30)^{5}
\]</span></p>
<p><span class="math display">\[
P(X = 15) = \frac{20 \times 19 \times 18 \times 17 \times 16}{5 \times 4 \times 3 \times 2 \times 1} (0.70)^{15} (0.30)^{5}
\]</span></p>
<p><span class="math display">\[
P(X = 15) = 15504 \times (0.70)^{15} (0.30)^{5}
\]</span></p>
<p><span class="math display">\[
P(X = 15) \approx 0.1073
\]</span></p>
<p>Therefore, the probability that exactly 15 of the 20 selected students will pass the course on their first attempt is approximately 0.1073.</p>
</div>
</div>
<div id="testing-large-samples" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Testing Large Samples<a href="testing-and-estimation---large-vs.-small-samples.html#testing-large-samples" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When dealing with large samples, we often utilize normal approximation to the binomial distribution due to the Central Limit Theorem (CLT). The CLT states that the sampling distribution of the sample mean will be approximately normally distributed for large sample sizes, regardless of the distribution of the population.</p>
<div id="z-test-for-proportions" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Z-Test for Proportions<a href="testing-and-estimation---large-vs.-small-samples.html#z-test-for-proportions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In testing large samples for binomial distributions, we typically employ the Z-test for proportions. This test allows us to determine whether the proportion of successes in our sample significantly differs from a hypothesized value.</p>
<p>The formula for the Z-test statistic is:</p>
<p><span class="math display">\[ Z = \frac{{\hat{p} - p_0}}{{\sqrt{\frac{{p_0(1 - p_0)}}{n}}}} \]</span></p>
<p>Where:
- <span class="math inline">\(\hat{p}\)</span> is the sample proportion,
- <span class="math inline">\(p_0\)</span> is the hypothesized proportion under the null hypothesis, and
- <span class="math inline">\(n\)</span> is the sample size.</p>
<p><strong>Binomial Testing Example</strong></p>
<p>Suppose in a university campus, it is believed that 60% of the students own a laptop. To test this claim, a random sample of 200 students is taken, and it is found that 115 of them own a laptop.</p>
<p>We want to test whether the proportion of students owning a laptop is significantly different from the claimed proportion of 60%.</p>
<p><strong>Hypotheses</strong></p>
<p>Let <span class="math inline">\(p\)</span> be the true proportion of students owning a laptop.</p>
<p>The null hypothesis (<span class="math inline">\(H_0\)</span>) is that <span class="math inline">\(p = 0.60\)</span>, and the alternative hypothesis (<span class="math inline">\(H_1\)</span>) is that <span class="math inline">\(p \neq 0.60\)</span>.</p>
<p><strong>Test Statistic</strong></p>
<p>We will use the Z-test for proportions to test the hypotheses. The test statistic is given by:</p>
<p><span class="math display">\[
Z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1 - p_0)}{n}}}
\]</span></p>
where:
<p>Given:
<span class="math display">\[\begin{align*}
    \hat{p} &amp;= \frac{115}{200} = 0.575 \\
    p_0 &amp;= 0.60 \\
    n &amp;= 200
\end{align*}\]</span></p>
<p>Using these values, we can calculate the test statistic:</p>
<p><span class="math display">\[
Z = \frac{0.575 - 0.60}{\sqrt{\frac{0.60(1 - 0.60)}{200}}}
\]</span></p>
<p><span class="math display">\[
Z = \frac{-0.025}{\sqrt{\frac{0.60(0.40)}{200}}}
\]</span></p>
<p><span class="math display">\[
Z \approx \frac{-0.025}{\sqrt{\frac{0.24}{200}}}
\]</span></p>
<p><span class="math display">\[
Z \approx \frac{-0.025}{\sqrt{0.0012}}
\]</span></p>
<p><span class="math display">\[
Z \approx \frac{-0.025}{0.0346}
\]</span></p>
<p><span class="math display">\[
Z \approx -0.7225
\]</span></p>
<p><strong>Conclusion</strong></p>
<p>The calculated value of the test statistic is <span class="math inline">\(Z = -0.7225\)</span>.</p>
<p>At a significance level of 0.05 (or 95% confidence level), the critical values for a two-tailed test are <span class="math inline">\(\pm 1.96\)</span>.</p>
<p>Since <span class="math inline">\(-1.96 &lt; -0.7225 &lt; 1.96\)</span>, we fail to reject the null hypothesis.</p>
<p>Therefore, there is not enough evidence to conclude that the proportion of students owning a laptop is significantly different from 60%.</p>
</div>
</div>
<div id="estimation-of-parameters" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Estimation of Parameters<a href="testing-and-estimation---large-vs.-small-samples.html#estimation-of-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="point-estimation-1" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Point Estimation<a href="testing-and-estimation---large-vs.-small-samples.html#point-estimation-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Point estimation involves estimating a single value for a population parameter based on sample data. For the binomial distribution, we often estimate the population proportion <span class="math inline">\(p\)</span> using the sample proportion <span class="math inline">\(\hat{p}\)</span>.</p>
</div>
<div id="interval-estimation" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Interval Estimation<a href="testing-and-estimation---large-vs.-small-samples.html#interval-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Interval estimation, on the other hand, provides a range of values within which the population parameter is likely to lie. For large samples, the confidence interval for the population proportion can be calculated using the normal approximation:</p>
<p><span class="math display">\[ \hat{p} \pm Z_{\alpha/2} \times \sqrt{\frac{{\hat{p}(1 - \hat{p})}}{n}} \]</span></p>
<p>Where <span class="math inline">\(Z_{\alpha/2}\)</span> is the critical value from the standard normal distribution corresponding to the desired level of confidence.</p>
<p><strong>Example</strong></p>
<p>Suppose in a university campus, it is believed that 60% of the students own a laptop. To estimate the true proportion of students owning a laptop, a random sample of 200 students is taken, and it is found that 115 of them own a laptop.</p>
<p>We want to construct a 95% confidence interval for the true proportion of students owning a laptop.</p>
<p><strong>Interval Estimate</strong></p>
<p>For large samples, the confidence interval for the population proportion can be calculated using the normal approximation:</p>
<p><span class="math display">\[
\hat{p} \pm Z_{\alpha/2} \times \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}
\]</span></p>
where:
<p>Given:
<span class="math display">\[\begin{align*}
    \hat{p} &amp;= 0.575 \\
    n &amp;= 200 \\
    \alpha &amp;= 0.05
\end{align*}\]</span></p>
<p>At a 95% confidence level, <span class="math inline">\(Z_{\alpha/2} = 1.96\)</span>.</p>
<p>Plugging in the values:</p>
<p><span class="math display">\[
\text{Margin of Error} = 1.96 \times \sqrt{\frac{0.575(1 - 0.575)}{200}} \approx 0.0704
\]</span></p>
<p><span class="math display">\[
\text{Lower Bound} = 0.575 - 0.0704 \approx 0.5046
\]</span></p>
<p><span class="math display">\[
\text{Upper Bound} = 0.575 + 0.0704 \approx 0.6446
\]</span></p>
<p>Therefore, the 95% confidence interval for the true proportion of students owning a laptop is approximately <span class="math inline">\((0.5046, 0.6446)\)</span>.</p>
</div>
</div>
<div id="small-sample-situations" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Small Sample Situations<a href="testing-and-estimation---large-vs.-small-samples.html#small-sample-situations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In small sample situations, the normal approximation may not be appropriate. Instead, we rely on exact methods such as the binomial test or the chi-square goodness-of-fit test.</p>
<div id="binomial-test" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Binomial Test<a href="testing-and-estimation---large-vs.-small-samples.html#binomial-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The binomial test is used to assess whether the observed number of successes in a sample significantly differs from the expected number of successes under a specified null hypothesis. It directly calculates the probability of observing the given number of successes or more extreme outcomes.</p>
<p><strong>Example</strong></p>
<p>Suppose in a university campus, it is believed that 60% of the students own a laptop. To test this claim, a random sample of 20 students is taken, and it is found that 11 of them own a laptop.</p>
<p>We want to test whether the proportion of students owning a laptop is significantly different from the claimed proportion of 60%.</p>
<p><strong>Hypotheses</strong></p>
<p>Let <span class="math inline">\(p\)</span> be the true proportion of students owning a laptop.</p>
<p>The null hypothesis (<span class="math inline">\(H_0\)</span>) is that <span class="math inline">\(p = 0.60\)</span>, and the alternative hypothesis (<span class="math inline">\(H_1\)</span>) is that <span class="math inline">\(p \neq 0.60\)</span>.</p>
<p><strong>Test Statistic</strong></p>
<p>For small sample situations, we can use the binomial test to assess whether the observed number of successes in a sample significantly differs from the expected number of successes under the null hypothesis.</p>
<p>The test statistic for the binomial test is the number of successes (<span class="math inline">\(k\)</span>).</p>
<p><strong>Calculation</strong></p>
<p>Given:
<span class="math display">\[\begin{align*}
    k &amp;= 11 \\
    n &amp;= 20 \\
    p_0 &amp;= 0.60
\end{align*}\]</span></p>
<p>Using these values, we can calculate the probability of observing 11 or fewer successes under the null hypothesis:</p>
<p><span class="math display">\[
P(X \leq 11) = \sum_{x=0}^{11} \binom{20}{x} (0.60)^x (0.40)^{20-x}
\]</span></p>
<p>Using statistical software or tables, we find <span class="math inline">\(P(X \leq 11) \approx 0.049\)</span>.</p>
<p>Since <span class="math inline">\(P(X \leq 11) &lt; 0.05\)</span>, we reject the null hypothesis.</p>
<p>The observed proportion of students owning a laptop is significantly different from the claimed proportion of 60%.</p>
<p><strong>Binomial Testing Example</strong></p>
<p>Suppose in a university campus, it is believed that 60% of the students own a laptop. To test this claim, a random sample of 200 students is taken, and it is found that 115 of them own a laptop.</p>
<p>We want to test whether the proportion of students owning a laptop is significantly different from the claimed proportion of 60%.</p>
<p><strong>Hypotheses</strong></p>
<p>Let <span class="math inline">\(p\)</span> be the true proportion of students owning a laptop.</p>
<p>The null hypothesis (<span class="math inline">\(H_0\)</span>) is that <span class="math inline">\(p = 0.60\)</span>, and the alternative hypothesis (<span class="math inline">\(H_1\)</span>) is that <span class="math inline">\(p \neq 0.60\)</span>.</p>
<p><strong>Test Statistic</strong></p>
<p>We will use the Z-test for proportions to test the hypotheses. The test statistic is given by:</p>
<p><span class="math display">\[
Z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1 - p_0)}{n}}}
\]</span></p>
where:
<p>Given:
<span class="math display">\[\begin{align*}
    \hat{p} &amp;= \frac{115}{200} = 0.575 \\
    p_0 &amp;= 0.60 \\
    n &amp;= 200
\end{align*}\]</span></p>
<p>Using these values, we can calculate the test statistic:</p>
<p><span class="math display">\[
Z = \frac{0.575 - 0.60}{\sqrt{\frac{0.60(1 - 0.60)}{200}}}
\]</span></p>
<p><span class="math display">\[
Z = \frac{-0.025}{\sqrt{\frac{0.60(0.40)}{200}}}
\]</span></p>
<p><span class="math display">\[
Z \approx \frac{-0.025}{\sqrt{\frac{0.24}{200}}}
\]</span></p>
<p><span class="math display">\[
Z \approx \frac{-0.025}{\sqrt{0.0012}}
\]</span></p>
<p><span class="math display">\[
Z \approx \frac{-0.025}{0.0346}
\]</span></p>
<p><span class="math display">\[
Z \approx -0.7225
\]</span></p>
<p><strong>Conclusion</strong></p>
<p>The calculated value of the test statistic is <span class="math inline">\(Z = -0.7225\)</span>.</p>
<p>At a significance level of 0.05 (or 95% confidence level), the critical values for a two-tailed test are <span class="math inline">\(\pm 1.96\)</span>.</p>
<p>Since <span class="math inline">\(-1.96 &lt; -0.7225 &lt; 1.96\)</span>, we fail to reject the null hypothesis.</p>
<p>Therefore, there is not enough evidence to conclude that the proportion of students owning a laptop is significantly different from 60%.</p>
</div>
<div id="chi-square-goodness-of-fit-test" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Chi-Square Goodness-of-Fit Test<a href="testing-and-estimation---large-vs.-small-samples.html#chi-square-goodness-of-fit-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The chi-square goodness-of-fit test is another method for testing the fit of observed data to an expected distribution. It can be used to compare the observed frequencies in different categories with the expected frequencies.</p>
<p><strong>Example</strong></p>
<p>Suppose in a university campus, it is believed that 60% of the students own a laptop. To test this claim, a random sample of 20 students is taken, and it is found that 11 of them own a laptop.</p>
<p>We want to test whether the proportion of students owning a laptop follows the claimed proportion of 60%.</p>
<p><strong>Hypotheses</strong></p>
<p>Let <span class="math inline">\(p\)</span> be the true proportion of students owning a laptop.</p>
<p>The null hypothesis (<span class="math inline">\(H_0\)</span>) is that the observed proportions follow the claimed proportion of 60%, and the alternative hypothesis (<span class="math inline">\(H_1\)</span>) is that they do not.</p>
<p><strong>Test Statistic</strong></p>
<p>For the chi-square goodness-of-fit test, we compare the observed frequencies with the expected frequencies under the null hypothesis. The test statistic is given by:</p>
<p><span class="math display">\[
\chi^2 = \sum \frac{{(O_i - E_i)^2}}{{E_i}}
\]</span></p>
where:
<p>Given:
<span class="math display">\[\begin{align*}
    \text{Observed successes (students with laptops)} &amp;= 11 \\
    \text{Total sample size} &amp;= 20 \\
    \text{Expected successes (based on claimed proportion)} &amp;= 0.60 \times 20 = 12
\end{align*}\]</span></p>
<p>Using these values, we can calculate the chi-square test statistic:</p>
<p><span class="math display">\[
\chi^2 = \frac{{(11 - 12)^2}}{{12}} = \frac{{(-1)^2}}{{12}} = \frac{1}{12} \approx 0.0833
\]</span></p>
<p><strong>Conclusion</strong></p>
<p>At a significance level of 0.05, with 1 degree of freedom, the critical value for the chi-square distribution is approximately 3.841.</p>
<p>Since <span class="math inline">\(\chi^2 = 0.0833 &lt; 3.841\)</span>, we fail to reject the null hypothesis.</p>
<p>Therefore, we do not have enough evidence to conclude that the observed proportions significantly differ from the claimed proportion of 60%.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="neyman-pearson-lemma.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="understanding-the-poisson-distribution.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/xadex01/Inferential_Statistics/edit/main/03-race.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/xadex01/Inferential_Statistics/blob/main/03-race.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
